{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13657420-d90a-49c3-843c-b68eccebeda8",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d0cc92-b77e-4274-9bd0-3e813761c2d4",
   "metadata": {},
   "source": [
    "## 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eca3207a-268d-4d91-893e-90e63903447d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [05:13<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "#Create text file with links \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "anime = []\n",
    "\n",
    "for page in tqdm(range(0, 400)):\n",
    "    url = 'https://myanimelist.net/topanime.php?limit=' + str(page * 50)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for tag in soup.find_all('tr'):\n",
    "        links = tag.find_all('a')\n",
    "        for link in links:        \n",
    "            if type(link.get('id')) == str and len(link.contents[0]) > 1:\n",
    "                anime.append((link.contents[0], link.get('href')) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eac08f9-f113-40de-b5ae-49bb13d32216",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"links.txt\", \"w\")\n",
    "for el in anime:\n",
    "    f.write(str(el[1])+'\\n')\n",
    "f.close()\n",
    "#We get 19083 rows, which implies we have less than 20000 animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ca4fc-d6a3-4868-af1c-b4c13f6da5e8",
   "metadata": {},
   "source": [
    "## 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33b42b20-a584-4c50-b9bc-89cec52348cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 3/18 [06:08<28:42, 114.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last well executed page is 366\n",
      "Last well executed page is 366\n",
      "Last well executed page is 366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 17/18 [25:00<01:28, 88.29s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-38da25c8909d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#Download the pages\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from random import randint\n",
    "\n",
    "#first open file from which we get links of the pages to download\n",
    "f = open(\"links.txt\",\"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "for page in tqdm(range(364,382)): #range is for pages to download [each page contains 50 articles]\n",
    "    #Manage the directories\n",
    "    os.system(\"mkdir Page\"+str(page+1)) #the (page+1) is to make so we don't have the first element 0 but 1\n",
    "    os.chdir(\"Page\"+str(page+1))\n",
    "    \n",
    "    #this is needed for the indexing of the articles\n",
    "    start=50*page\n",
    "    \n",
    "    for i in range(start,start+50):\n",
    "        url=lines[i]\n",
    "        check = requests.get(url)\n",
    "        \n",
    "        #Check if the site is blocking the download, start again when we are allowed to\n",
    "        if check.status_code != 200:\n",
    "            print(\"Last well executed page is {}\".format(page-1))\n",
    "            while(check.status_code != 200): #Stop while we can't download\n",
    "                check = requests.get(url)    #update the request to connect to the page until\n",
    "                                             #we manage to connect\n",
    "                    \n",
    "                #next line was just a try of changing the ip when the site would stop the downloads\n",
    "                #in short didn't manage to implement it\n",
    "                #os.system((\"sudo ifconfig lo 127.0.0.{}  netmask 255.0.0.0\").format(randint(0,50))) \n",
    "       \n",
    "        #Save the content of the page \n",
    "        art=open(\"article\"+str(i+1)+'.html',\"wb\")\n",
    "        art.write(check.content)\n",
    "        art.close()\n",
    "    \n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce311ac6-f9d3-4f29-9143-c73e5812e236",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "84f0fe8d-3c30-4fef-b565-edcf8711c517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "#function that parses information from the single article \n",
    "def Parse2(tsv, soup):\n",
    "    animeTitle,animeType,(animeNumEpisode),(releaseDate),(endDate),(animeNumMembers),(animeScore),(animeUsers),(animeRank),(animePopularity),animeDescription,(animeRelated),(animeCharacters),(animeVoices),(animeStaff)=\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n",
    "\n",
    "    left_col=soup.find('div', {'style':\"width: 225px\"})\n",
    "    l_div=left_col.find_all(\"div\",{\"class\":\"spaceit_pad\"}) #we will iterate over this, more later\n",
    "    \n",
    "    start=\"\"\n",
    "    end=\"\"\n",
    "    \n",
    "    #This is a for over the \"div tag\" on the left column of the page, in which are listed most of the info we care about\n",
    "    #going through the html like this speeds up the process of reading the single element\n",
    "    for i in range(len(l_div)):\n",
    "        \n",
    "        if \"Type\" in l_div[i].getText(): #get type\n",
    "            animeType = l_div[i].getText().replace(\"Type:\",\"\").replace(\"\\n\",\"\").strip()\n",
    "            #print(animeType)\n",
    "            \n",
    "        if \"Episodes\" in l_div[i].getText(): #get #episodes\n",
    "            animeNumEpisode = (l_div[i].getText().replace(\"Episodes:\",\"\").replace(\"\\n\",\"\").strip())\n",
    "            if animeNumEpisode==\"Unknown\":\n",
    "                animeNumEpisode=\"\"\n",
    "            #print(animeNumEpisode)\n",
    "        \n",
    "        #this piece of code is probably not that readable but I had to adjust it in different times in order\n",
    "        #to get as much info as possible since there were many different cases\n",
    "        if \"Aired\" in l_div[i].getText(): #get end and starting date (when it's there)\n",
    "            if len(l_div[i].getText().replace(\"Aired:\",\"\").replace(\"to\",\"\").strip().split())==6:\n",
    "                start=\" \".join(l_div[i].getText().replace(\"Aired:\",\"\").replace(\",\",\"\").replace(\"to\",\"\").strip().split()[0:3])\n",
    "                end=\" \".join(l_div[i].getText().replace(\"Aired:\",\"\").replace(\",\",\"\").replace(\"to\",\"\").strip().split()[3:6])\n",
    "                start=datetime.strptime(start,\"%b %d %Y\")\n",
    "                end=datetime.strptime(end,\"%b %d %Y\")\n",
    "            if len(l_div[i].getText().replace(\"Aired:\",\"\").replace(\",\",\"\").replace(\"to\",\"\").strip().split())==3 and (\"?\" not in l_div[i].getText().replace(\"Aired:\",\"\").replace(\"to\",\"\").strip().split()):\n",
    "                if len(l_div[i].getText().replace(\"Aired:\",\"\").replace(\",\",\"\").replace(\"to\",\"\").strip().split()[1])!=len(l_div[i].getText().replace(\"Aired:\",\"\").replace(\",\",\"\").replace(\"to\",\"\").strip().split()[2]):\n",
    "                    \n",
    "                    start=\" \".join(l_div[i].getText().replace(\"Aired:\",\"\").replace(\",\",\"\").replace(\"to\",\"\").strip().split()[0:3])\n",
    "                    start=datetime.strptime(start,\"%b %d %Y\")\n",
    "                else:\n",
    "                    start=\"\"\n",
    "            else:\n",
    "                start=end=\"\"\n",
    "            \n",
    "            releaseDate=start\n",
    "            endDate=end\n",
    "            #print(releaseDate)\n",
    "            #print(endDate)\n",
    "            \n",
    "        #the i!=0 and \"ookiku\" exceptions are ad-hoc exceptions made for some animes\n",
    "        #which contained info parsed in a different way from the rest of the group\n",
    "        if (\"Score\" in l_div[i].getText()) and (i!=0) and (\"Ookiku\" not in l_div[i].getText()): #get score\n",
    "            if l_div[i].getText().replace(\"Score:\",\"\").replace(\",\",\"\").split()[3]!=\"-\":\n",
    "                animeUsers = int(l_div[i].getText().replace(\"Score:\",\"\").replace(\",\",\"\").split()[3])\n",
    "                animeScore = float(l_div[i].getText().replace(\"Score:\",\"\").replace(\",\",\"\").split()[0])\n",
    "            #print(animeUsers)\n",
    "            #print(animeScore)\n",
    "        #if \"Ranked\" in l_div[i].getText():#19 >>>> conviene prenderlo da sopra il rank\n",
    "            \n",
    "        if \"Popularity\" in l_div[i].getText(): #get popularity\n",
    "            animePopularity = int(l_div[i].getText().replace(\"Popularity:\",\"\").strip().replace(\"#\",\"\"))\n",
    "            #print(animePopularity)\n",
    "    \n",
    "        if \"Members\" in l_div[i].getText(): #get #members\n",
    "            animeNumMembers = int(l_div[i].getText().replace(\"Members:\",\"\").strip().replace(\",\",\"\"))\n",
    "            #print(animeNumMembers)\n",
    "   \n",
    "    \n",
    "    description=soup.find(\"p\",{\"itemprop\":\"description\"}) #get synopsis\n",
    "    animeDescription=description.getText().replace(\"\\n\",\"\")\n",
    "    #print(animeDescription)\n",
    "    \n",
    "    #THESE ARE THE SUGGESTED ONES (WE WANT THE RELATED)\n",
    "    \"\"\"rel=[]\n",
    "    related=soup.find_all(\"li\",{\"class\":\"btn-anime\",\"style\":\"width:90px\"})\n",
    "    for i in range(len(related)):\n",
    "        rel.append(related[i][\"title\"])\n",
    "    animeRelated = rel\n",
    "    print(animeRelated)\"\"\"\n",
    "     \n",
    "    rel=[] #get related animes\n",
    "    related=soup.find(\"table\",{\"class\":\"anime_detail_related_anime\"})#.find_all(\"td\",{\"class\":\"borderClass\"})\n",
    "    if (related)!=None:\n",
    "        #print(type(related),related)\n",
    "        related=related.find_all(\"td\",{\"class\":\"borderClass\"})\n",
    "        for i in range(len(related)):\n",
    "            if i%2!=0:\n",
    "                rel.append(related[i].getText())\n",
    "    animeRelated=rel\n",
    "    \n",
    "    \n",
    "    chars=[]\n",
    "    voices=[]\n",
    "    staff=[]\n",
    "    l=[]\n",
    "    \n",
    "    #As before, loop over the 2 tables in the html to speed up everything\n",
    "    seed=soup.find_all(\"div\",{\"class\":\"detail-characters-list clearfix\"})\n",
    "    for i in range(len(seed)):\n",
    "        if i==0:\n",
    "            who=soup.find(\"div\",{\"class\":\"detail-characters-list clearfix\"}).find_all(\"img\")\n",
    "            for i in range(len(who)): #get anime characters and voices\n",
    "                if i%2==0:\n",
    "                    chars.append(\",\".join(who[i][\"alt\"].replace(\" \",\"\").split(\",\")))\n",
    "                else:\n",
    "                    voices.append(\",\".join(who[i][\"alt\"].replace(\" \",\"\").split(\",\")))\n",
    "    \n",
    "        if i==1: #get anime staff\n",
    "            if (len(soup.find_all(\"div\",{\"class\":\"detail-characters-list clearfix\"}))>1):\n",
    "                for i in range(len(soup.find_all(\"div\",{\"class\":\"detail-characters-list clearfix\"})[1].find_all(\"img\"))):\n",
    "                    staff.append(soup.find_all(\"div\",{\"class\":\"detail-characters-list clearfix\"})[1].find_all(\"img\")[i][\"alt\"])\n",
    "                    staff.append((soup.find_all(\"div\",{\"class\":\"detail-characters-list clearfix\"})[1].find_all(\"div\", {\"class\":\"spaceit_pad\"}))[i].getText().strip())\n",
    "                    \n",
    "                for i in range(0,2*len(soup.find_all(\"div\",{\"class\":\"detail-characters-list clearfix\"})[1].find_all(\"img\")),2):\n",
    "                    l.append(staff[i:i+2])\n",
    "            \n",
    "            \n",
    "    animeCharacters = chars\n",
    "    animeVoices = voices\n",
    "    animeStaff = l \n",
    "    #print(animeCharacters)\n",
    "    #print(animeVoices)\n",
    "    #print(animeStaff)\n",
    "    \n",
    "    \n",
    "    animeTitle = soup.find(\"h1\",{\"class\":\"title-name h1_bold_none\"}).getText() #get title\n",
    "    animeRank=soup.find(\"span\",{\"class\":\"numbers ranked\"}).getText().replace(\"Ranked #\",\"\") #get rank\n",
    "    #Here we exclude the missing information case\n",
    "    if \"N/A\" not in animeRank:\n",
    "        animeRank = int(animeRank)\n",
    "    else:\n",
    "        animeRank=\"\"\n",
    "    \n",
    "    #write title and info in the TSV\n",
    "    tsv.write(\"animeTitle\"+\"\\t\"+\"animeType\"+\"\\t\"+\"animeNumEpisode\"+\"\\t\"+\"releaseDate\"+\"\\t\"+\"endDate\"+\"\\t\"+\"animeNumMembers\"+\"\\t\"+\"animeScore\"+\"\\t\"+\"animeUsers\"+\"\\t\"+\"animeRank\"+\"\\t\"+\"animePopularity\"+\"\\t\"+\"animeDescription\"+\"\\t\"+\"animeRelated\"+\"\\t\"+\"animeCharacters\"+\"\\t\"+\"animeVoices\"+\"\\t\"+\"animeStaff\"+\"\\n\")\n",
    "    tsv.write(animeTitle+\"\\t\"+animeType+\"\\t\"+str(animeNumEpisode)+\"\\t\"+str(releaseDate)+\"\\t\"+str(endDate)+\"\\t\"+str(animeNumMembers)+\"\\t\"+str(animeScore)+\"\\t\"+str(animeUsers)+\"\\t\"+str(animeRank)+\"\\t\"+str(animePopularity)+\"\\t\"+animeDescription+\"\\t\"+str(animeRelated)+\"\\t\"+str(animeCharacters)+\"\\t\"+str(animeVoices)+\"\\t\"+str(animeStaff))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "29c3bddf-4812-46ca-9953-557a65a7827e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 381/382 [38:20<00:06,  6.04s/it] \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'article19084.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-1b97c06980e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#Open the articles of the page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Page\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"article\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.html'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TSV\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'article19084.html'"
     ]
    }
   ],
   "source": [
    "#PARSING\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "for page in tqdm(range(0,382)):\n",
    "    \n",
    "    start=50*page\n",
    "    \n",
    "    for i in range(start,start+50):\n",
    "        #Open the articles of the page and save all the TSVs in a single directory\n",
    "        os.chdir(\"Page\"+str(page+1))\n",
    "        art=open(\"article\"+str(i+1)+'.html',\"r\")\n",
    "        os.chdir(\"..\")\n",
    "        os.chdir(\"TSV\")\n",
    "        tsv=open(\"anime_\"+str(i+1)+\".tsv\",\"w\")\n",
    "        \n",
    "        soup=BeautifulSoup(art.read(), \"html.parser\") \n",
    "        \n",
    "        #Just use the parsing function we developed earlier\n",
    "        Parse2(tsv,soup)\n",
    "        \n",
    "        art.close()\n",
    "        tsv.close()\n",
    "        os.chdir(\"..\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ff8989-abcf-4b0e-8297-0d06348fb60b",
   "metadata": {},
   "source": [
    "# Part 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4178e6ca-124f-49ed-bb4f-1756282e4a4f",
   "metadata": {},
   "source": [
    "## 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee797d-f8a8-49d6-b320-50929dafbd67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Pseudocode:\n",
    "\"\"\"\n",
    "A: array of length l of which we want to compute the maximum sum of non adjacent elements\n",
    "\n",
    "def compute(A): \n",
    "    l=len(A)       \n",
    "    gain=[]      declare 2 arrays: one to store the values of the gain at each iteration and the other \n",
    "    path=[]                        to use later to save only the elements of the gain we care about (the path chosen\n",
    "                                   to get the sum)\n",
    "    inc=exc=0    here we initialize the variables that will keep inside them the value of the 2 sums at each iteration:\n",
    "                 one including and one excluding the single element \n",
    "                 \n",
    "    for i in range(l):          then we just compute the value of the 2 sums at each iteration and save the gain \n",
    "        temp=inc                for each of them\n",
    "        inc=max(inc,A[i]+exc)\n",
    "        exc=temp            \n",
    "        gain[i]=(inc-exc)\n",
    "    \n",
    "    for j in range(0,len(gain)-1,2):  \n",
    "        if gain[j+1]>0:               here we compare the elements of the gain list and take only those of interest:\n",
    "            path[j]=(A[j+1])          if the gain is equal to 0 for that iteration the value is excluded \"a priori\"\n",
    "        else:                         then we impose the condition to only get alternate values for which the gain is\n",
    "            path[j]=(A[j])            greater than zero\n",
    "    \n",
    "    return sum(path),path\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20cefbb-da56-47e3-adff-6742966783be",
   "metadata": {},
   "source": [
    "## 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "5a9905eb-ecd5-42ef-91b7-70c5eb38f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation\n",
    "def compute(A):\n",
    "    l=len(A)\n",
    "    gain=[]\n",
    "    path=[]\n",
    "    inc=exc=0 #include, exclude --> compute value of sum including or excluding the i-th element\n",
    "\n",
    "    for i in range(l):\n",
    "        temp=inc\n",
    "        inc=max(inc,A[i]+exc)\n",
    "        exc=temp            \n",
    "        gain.append(inc-exc)#save the gain\n",
    "\n",
    "    \n",
    "    for j in range(0,len(gain)-1,2):\n",
    "        if gain[j+1]>0:\n",
    "            path.append(A[j+1])\n",
    "        else:\n",
    "            path.append(A[j])\n",
    "    \n",
    "    return sum(path),path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "61320f62-b409-4bb4-90a8-3e7e32ddecf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110, [40, 50, 20])"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=[30, 40, 25, 50, 30, 20]\n",
    "compute(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc5474-e481-4096-a45b-090394aea9b6",
   "metadata": {},
   "source": [
    "Given an array A of length $n$ the complexity of the algorithm is $O(n)$ [excluding constants].\\\n",
    "It took me over 8 hours to come up with this solution (I had problems with the implementation), in which I tried different approaches to the problem:\n",
    "1. I first tried looking at the list as a tree, but computing every possible case to then take the one with the highest output would lead me to a solution of order $O(n^2)$.\n",
    "1. After this attempt I started searching on the internet and came across this video (https://www.youtube.com/watch?v=UtGtF6nc35g) in which the inclusion/exclusion method is explained. Making few additions to that algorithm I managed to get a result that is able to find the highest value solution along with the chosen elements of the starting array that seems to work in all the cases and in $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd355b5-f0a8-4f79-85a7-408e16a402e2",
   "metadata": {},
   "source": [
    "# Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75bc23bb-0e69-4a32-9cbd-9ebe2a80597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "def saveReviews(rev,soup):\n",
    "    revs=soup.find_all(\"div\",{\"class\":\"spaceit textReadability word-break pt8 mt8\",\n",
    "                     \"style\":\"clear: both; border-top: 1px solid #ebebeb;\"})\n",
    "    if len(revs)>1:\n",
    "        for i in range(len(revs)):\n",
    "            single_rev=revs[i].getText().strip().replace(\"\\n\",\"\")[80:580]\n",
    "            rev.write(single_rev+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e46ccf1-6a25-4500-b2a7-98516684c7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/382 [00:09<1:01:39,  9.71s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1a1024d87d33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mrev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"review_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0msoup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#Just use the parsing function we developed earlier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m                 \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m         \u001b[0;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/bs4/builder/_htmlparser.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTMLParseError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/html/parser.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoahead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/html/parser.py\u001b[0m in \u001b[0;36mgoahead\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_charrefs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdata_elem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrawdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "for page in tqdm(range(0,382)):\n",
    "\n",
    "    start=50*page\n",
    "\n",
    "    for i in range(start,start+50):\n",
    "        #Open the articles of the page and save all the TSVs in a single directory\n",
    "        os.chdir(\"Page\"+str(page+1))\n",
    "        art=open(\"article\"+str(i+1)+'.html',\"r\")\n",
    "        os.chdir(\"..\")\n",
    "        os.chdir(\"Reviews\")\n",
    "        rev=open(\"review_\"+str(i+1)+\".txt\",\"w\")\n",
    "\n",
    "        soup=BeautifulSoup(art.read(), \"html.parser\") \n",
    "\n",
    "        #Just use the parsing function we developed earlier\n",
    "        saveReviews(rev,soup)\n",
    "\n",
    "        art.close()\n",
    "        rev.close()\n",
    "        os.chdir(\"..\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54917979-70ff-497b-aaea-ecc5353813c7",
   "metadata": {},
   "source": [
    "12002 out of the 19083 txt files just created are empty, which means most of the animes don't have a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "043fa8c6-fa41-47c8-8147-9850fac178d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/simone/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0ad5eb7-6339-4b20-a42d-3402b62dd3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(string):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    string=tokenizer.tokenize(string)\n",
    "    for word in range(len(string)):\n",
    "        string[word] = string[word].lower() \n",
    "    #Remove stopwords\n",
    "    string = [word for word in string if not word in stopwords.words()]\n",
    "    #STEMMING\n",
    "    stemmer = PorterStemmer()\n",
    "    string = [stemmer.stem(word) for word in string]\n",
    "    return str(\" \".join(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1108e6-8a16-474c-9288-74b0e65aacb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19083/19083 [2:19:37<00:00,  2.28it/s]  \n"
     ]
    }
   ],
   "source": [
    "art_comment=[]\n",
    "os.chdir(\"Reviews\")\n",
    "comments=[]\n",
    "art_comment=[]\n",
    "for page in tqdm(range(0,19083)):\n",
    "    if os.stat(\"review_\"+str(page+1)+\".txt\").st_size != 0:\n",
    "        rev=open(\"review_\"+str(page+1)+\".txt\",\"r\")\n",
    "        lines=rev.readlines()\n",
    "        for i in range(len(lines)):\n",
    "            art_comment.append(clean(lines[i]))\n",
    "        comments.append(str(art_comment)+\"\\n\")\n",
    "        art_comment=[]\n",
    "        rev.close()\n",
    "    else:\n",
    "        comments.append(\"\\t\\n\")\n",
    "    \n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3578ef16-78c1-43c8-8e14-c0d4cac33789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19083"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d18c7cfc-7fbc-48f1-a28e-cbf4b844214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b07d4d2d-9599-4012-b6e2-95e9d90d185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results\n",
    "os.chdir(\"Reviews\")\n",
    "c=open(\"comments.txt\",\"w\")\n",
    "for comment in comments:\n",
    "    c.write(str(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20937b5c-34ec-48ef-84f3-2f9e49d6e4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load results\n",
    "comments=[]\n",
    "os.chdir(\"Reviews\")\n",
    "c=open(\"comments.txt\",\"r\")\n",
    "lines=c.readlines()\n",
    "for i in range(len(lines)):\n",
    "    comments.append(lines[i].replace(\"\\n\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",'').split(\",\"))\n",
    "c.close()\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a00a5a51-4a6e-45f3-be6e-c8b2202f2986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' fullmet alchemist brotherhood get immens amount prais mal commun 1 rank show constantli refer masterpiec greatest show ever creat seen mani fan preach live hype never receiv much prais opinion guy certainli law land anyth howev person feel though call fma b masterpiec champion show bit'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(comments[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "591543d3-3d22-4c45-890a-7aeb5b983158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19083/19083 [00:03<00:00, 5904.91it/s] \n"
     ]
    }
   ],
   "source": [
    "#build corpus with the single words appearing in all reviews\n",
    "corpus={}\n",
    "#update=1\n",
    "for i in tqdm(range(len(comments))):\n",
    "    #print(\"i={}\".format(i))\n",
    "    for j in range(len(comments[i])):\n",
    "        sentence=comments[i][j].split(\" \")\n",
    "        \n",
    "        for word in sentence:\n",
    "            \n",
    "            if word not in corpus:\n",
    "                corpus.update({word:0})\n",
    "                #update=update+1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487ddb7b-cce1-4e32-8572-38237e516e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build vector with the count of words appearing in all reviews\n",
    "vectors=[]\n",
    "vec_sentence=[]\n",
    "doc=dict.fromkeys(corpus, 0)\n",
    "for i in tqdm(range(0,len(comments))):\n",
    "    vec_sentence=[]\n",
    "    for j in range(len(comments[i])):\n",
    "        \n",
    "        sentence=comments[i][j].split(\" \")\n",
    "        for word in sentence:\n",
    "            doc[word] += 1\n",
    "        \n",
    "        vec_sentence.append(list(doc.values()))\n",
    "        doc=dict.fromkeys(corpus, 0)\n",
    "    vectors.append(vec_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f12940f-caec-4b9a-8744-c55e8117a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save vectors\n",
    "v=open(\"Reviews/vectors.txt\",\"w\")\n",
    "for i in tqdm(range(len(vectors))):\n",
    "    for j in (range(len(vectors[i]))):\n",
    "        v.write(str(vectors[i][j])+\"\\t\")\n",
    "    v.write(\"\\n\")\n",
    "v.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30a07c73-32e4-4534-b0f9-88f2440ad55a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-23-c2dd2c030459>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-c2dd2c030459>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    v.close()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#load vectors\n",
    "vectors=[]\n",
    "temp_vec=[]\n",
    "v=open(\"Reviews/vectors.txt\",\"r\")\n",
    "lines=v.readlines()\n",
    "#for i in tqdm(range(0,len(lines))):\n",
    "    \n",
    "    \n",
    "v.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d524265-27e6-48d7-b727-5a1dcb554f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "v=open(\"Reviews/vectors.txt\",\"r\")\n",
    "lines=v.readlines()\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6d667b-ee39-4c14-adf0-0eec9df5c8cb",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6de52f87-57fe-42b7-aa1a-6c60ee4139cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#from kneed import KneeLocator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d0922edf-09a4-4c8e-a80a-048336a0907c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=np.array([0,1]) #negative=0, positive=1\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "900b2514-6c80-4474-9d36-49a5715c5f81",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[4. 3. 3.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-05a36f9750b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"random\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \"\"\"\n\u001b[0;32m--> 979\u001b[0;31m         X = self._validate_data(X, accept_sparse='csr',\n\u001b[0m\u001b[1;32m    980\u001b[0m                                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                                 \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'no_validation'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;31m# If input is 1D raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    638\u001b[0m                     \u001b[0;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[4. 3. 3.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(init=\"random\", n_clusters=2, n_init=10, max_iter=300, random_state=42)\n",
    "kmeans.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c99c5aa-2628-44e6-aee1-4994c1825a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19083"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs=open(\"vectors.txt\",\"r\")\n",
    "l=vecs.readlines()\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810ee454-f73b-475e-882b-1717f5acd07c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
