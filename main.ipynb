{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WF6pvUqgSryv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "import string\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import *\n",
    "nltk.download('stopwords')\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = []\n",
    "\n",
    "for page in tqdm(range(0, 400)):\n",
    "    url = 'https://myanimelist.net/topanime.php?limit=' + str(page * 50)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for tag in soup.find_all('tr'):\n",
    "        links = tag.find_all('a')\n",
    "        for link in links:        \n",
    "            if type(link.get('id')) == str and len(link.contents[0]) > 1:\n",
    "                anime.append((link.contents[0], link.get('href')) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"links.txt\", \"w\")\n",
    "for el in anime:\n",
    "    f.write(str(el[1])+'\\n')\n",
    "f.close()\n",
    "#We get 19083 rows, which implies we have less than 20000 animes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first open file from which we get links of the pages to download\n",
    "f = open(\"links.txt\",\"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "for page in tqdm(range(364,382)): #range is for pages to download [each page contains 50 articles]\n",
    "    #Manage the directories\n",
    "    os.system(\"mkdir Page\"+str(page+1)) #the (page+1) is to make so we don't have the first element 0 but 1\n",
    "    os.chdir(\"Page\"+str(page+1))\n",
    "    \n",
    "    #this is needed for the indexing of the articles\n",
    "    start=50*page\n",
    "    \n",
    "    for i in range(start,start+50):\n",
    "        url=lines[i]\n",
    "        check = requests.get(url)\n",
    "        \n",
    "        #Check if the site is blocking the download, start again when we are allowed to\n",
    "        if check.status_code != 200:\n",
    "            print(\"Last well executed page is {}\".format(page-1))\n",
    "            while(check.status_code != 200): #Stop while we can't download\n",
    "                check = requests.get(url)    #update the request to connect to the page until\n",
    "                                             #we manage to connect\n",
    "                    \n",
    "                #next line was just a try of changing the ip when the site would stop the downloads\n",
    "                #in short didn't manage to implement it\n",
    "                #os.system((\"sudo ifconfig lo 127.0.0.{}  netmask 255.0.0.0\").format(randint(0,50))) \n",
    "       \n",
    "        #Save the content of the page \n",
    "        art=open(\"article\"+str(i+1)+'.html',\"wb\")\n",
    "        art.write(check.content)\n",
    "        art.close()\n",
    "    \n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that parses information from the single article \n",
    "def Parse2(tsv, soup):\n",
    "    animeTitle,animeType,(animeNumEpisode),(releaseDate),(endDate),(animeNumMembers),(animeScore),(animeUsers),(animeRank),(animePopularity),animeDescription,(animeRelated),(animeCharacters),(animeVoices),(animeStaff)=\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n",
    "\n",
    "    left_col=soup.find('div', {'style':\"width: 225px\"})\n",
    "    l_div=left_col.find_all(\"div\",{\"class\":\"spaceit_pad\"}) #we will iterate over this, more later\n",
    "    \n",
    "    start=\"\"\n",
    "    end=\"\"\n",
    "    \n",
    "    #This is a for over the \"div tag\" on the left column of the page, in which are listed most of the info we care about\n",
    "    #going through the html like this speeds up the process of reading the single element\n",
    "    for i in range(len(l_div)):\n",
    "        \n",
    "        if \"Type\" in l_div[i].getText(): #get type\n",
    "            animeType = l_div[i].getText().replace(\"Type:\",\"\").replace(\"\\n\",\"\").strip()\n",
    "            #print(animeType)\n",
    "            \n",
    "        if \"Episodes\" in l_div[i].getText(): #get #episodes\n",
    "            animeNumEpisode = (l_div[i].getText().replace(\"Episodes:\",\"\").replace(\"\\n\",\"\").strip())\n",
    "            if animeNumEpisode==\"Unknown\":\n",
    "                animeNumEpisode=\"\"\n",
    "            #print(animeNumEpisode)\n",
    "        \n",
    "        #this piece of code is probably not that readable but I had to adjust it in different times in order\n",
    "        #to get as much info as possible since there were many different cases\n",
    "        if \"Aired\" in l_div[i].getText(): #get end and starting date (when it's there)\n",
    "            if len(l_div[i].getText().replace(\"Aired:\",\"\").replace(\"to\",\"\").strip().split())==6:\n",
    "                start=\" \".join(l_div[i].getText().replace(\"Aired:\",\"\").replace(\",\",\"\").replace(\"to\",\"\").strip().split()[0:3])\n",
    "                end=\" \".join(l_div[i].getText().replace(\"Aired:\",\"\").replace(\",\",\"\").replace(\"to\",\"\").strip().split()[3:6])\n",
    "                start=datetime.strptime(start,\"%b %d %Y\")\n",
    "                end=datetime.strptime(end,\"%b %d %Y\")\n",
    "            if len(l_div[i].getText().replace(\"Aired:\",\"\").replace(\",\",\"\").replace(\"to\",\"\").strip().split())==3 and (\"?\" not in l_div[i].getText().replace(\"Aired:\",\"\").replace(\"to\",\"\").strip().split()):\n",
    "                if len(l_div[i].getText().replace(\"Aired:\",\"\").replace(\",\",\"\").replace(\"to\",\"\").strip().split()[1])!=len(l_div[i].getText().replace(\"Aired:\",\"\").replace(\",\",\"\").replace(\"to\",\"\").strip().split()[2]):\n",
    "                    \n",
    "                    start=\" \".join(l_div[i].getText().replace(\"Aired:\",\"\").replace(\",\",\"\").replace(\"to\",\"\").strip().split()[0:3])\n",
    "                    start=datetime.strptime(start,\"%b %d %Y\")\n",
    "                else:\n",
    "                    start=\"\"\n",
    "            else:\n",
    "                start=end=\"\"\n",
    "            \n",
    "            releaseDate=start\n",
    "            endDate=end\n",
    "            #print(releaseDate)\n",
    "            #print(endDate)\n",
    "            \n",
    "        #the i!=0 and \"ookiku\" exceptions are ad-hoc exceptions made for some animes\n",
    "        #which contained info parsed in a different way from the rest of the group\n",
    "        if (\"Score\" in l_div[i].getText()) and (i!=0) and (\"Ookiku\" not in l_div[i].getText()): #get score\n",
    "            if l_div[i].getText().replace(\"Score:\",\"\").replace(\",\",\"\").split()[3]!=\"-\":\n",
    "                animeUsers = int(l_div[i].getText().replace(\"Score:\",\"\").replace(\",\",\"\").split()[3])\n",
    "                animeScore = float(l_div[i].getText().replace(\"Score:\",\"\").replace(\",\",\"\").split()[0])\n",
    "            #print(animeUsers)\n",
    "            #print(animeScore)\n",
    "        #if \"Ranked\" in l_div[i].getText():#19 >>>> conviene prenderlo da sopra il rank\n",
    "            \n",
    "        if \"Popularity\" in l_div[i].getText(): #get popularity\n",
    "            animePopularity = int(l_div[i].getText().replace(\"Popularity:\",\"\").strip().replace(\"#\",\"\"))\n",
    "            #print(animePopularity)\n",
    "    \n",
    "        if \"Members\" in l_div[i].getText(): #get #members\n",
    "            animeNumMembers = int(l_div[i].getText().replace(\"Members:\",\"\").strip().replace(\",\",\"\"))\n",
    "            #print(animeNumMembers)\n",
    "   \n",
    "    \n",
    "    description=soup.find(\"p\",{\"itemprop\":\"description\"}) #get synopsis\n",
    "    animeDescription=description.getText().replace(\"\\n\",\"\")\n",
    "    #print(animeDescription)\n",
    "    \n",
    "    #THESE ARE THE SUGGESTED ONES (WE WANT THE RELATED)\n",
    "    \"\"\"rel=[]\n",
    "    related=soup.find_all(\"li\",{\"class\":\"btn-anime\",\"style\":\"width:90px\"})\n",
    "    for i in range(len(related)):\n",
    "        rel.append(related[i][\"title\"])\n",
    "    animeRelated = rel\n",
    "    print(animeRelated)\"\"\"\n",
    "     \n",
    "    rel=[] #get related animes\n",
    "    related=soup.find(\"table\",{\"class\":\"anime_detail_related_anime\"})#.find_all(\"td\",{\"class\":\"borderClass\"})\n",
    "    if (related)!=None:\n",
    "        #print(type(related),related)\n",
    "        related=related.find_all(\"td\",{\"class\":\"borderClass\"})\n",
    "        for i in range(len(related)):\n",
    "            if i%2!=0:\n",
    "                rel.append(related[i].getText())\n",
    "    animeRelated=rel\n",
    "    \n",
    "    \n",
    "    chars=[]\n",
    "    voices=[]\n",
    "    staff=[]\n",
    "    l=[]\n",
    "    \n",
    "    #As before, loop over the 2 tables in the html to speed up everything\n",
    "    seed=soup.find_all(\"div\",{\"class\":\"detail-characters-list clearfix\"})\n",
    "    for i in range(len(seed)):\n",
    "        if i==0:\n",
    "            who=soup.find(\"div\",{\"class\":\"detail-characters-list clearfix\"}).find_all(\"img\")\n",
    "            for i in range(len(who)): #get anime characters and voices\n",
    "                if i%2==0:\n",
    "                    chars.append(\",\".join(who[i][\"alt\"].replace(\" \",\"\").split(\",\")))\n",
    "                else:\n",
    "                    voices.append(\",\".join(who[i][\"alt\"].replace(\" \",\"\").split(\",\")))\n",
    "    \n",
    "        if i==1: #get anime staff\n",
    "            if (len(soup.find_all(\"div\",{\"class\":\"detail-characters-list clearfix\"}))>1):\n",
    "                for i in range(len(soup.find_all(\"div\",{\"class\":\"detail-characters-list clearfix\"})[1].find_all(\"img\"))):\n",
    "                    staff.append(soup.find_all(\"div\",{\"class\":\"detail-characters-list clearfix\"})[1].find_all(\"img\")[i][\"alt\"])\n",
    "                    staff.append((soup.find_all(\"div\",{\"class\":\"detail-characters-list clearfix\"})[1].find_all(\"div\", {\"class\":\"spaceit_pad\"}))[i].getText().strip())\n",
    "                    \n",
    "                for i in range(0,2*len(soup.find_all(\"div\",{\"class\":\"detail-characters-list clearfix\"})[1].find_all(\"img\")),2):\n",
    "                    l.append(staff[i:i+2])\n",
    "            \n",
    "            \n",
    "    animeCharacters = chars\n",
    "    animeVoices = voices\n",
    "    animeStaff = l \n",
    "    #print(animeCharacters)\n",
    "    #print(animeVoices)\n",
    "    #print(animeStaff)\n",
    "    \n",
    "    \n",
    "    animeTitle = soup.find(\"h1\",{\"class\":\"title-name h1_bold_none\"}).getText() #get title\n",
    "    animeRank=soup.find(\"span\",{\"class\":\"numbers ranked\"}).getText().replace(\"Ranked #\",\"\") #get rank\n",
    "    #Here we exclude the missing information case\n",
    "    if \"N/A\" not in animeRank:\n",
    "        animeRank = int(animeRank)\n",
    "    else:\n",
    "        animeRank=\"\"\n",
    "    \n",
    "    #write title and info in the TSV\n",
    "    tsv.write(\"animeTitle\"+\"\\t\"+\"animeType\"+\"\\t\"+\"animeNumEpisode\"+\"\\t\"+\"releaseDate\"+\"\\t\"+\"endDate\"+\"\\t\"+\"animeNumMembers\"+\"\\t\"+\"animeScore\"+\"\\t\"+\"animeUsers\"+\"\\t\"+\"animeRank\"+\"\\t\"+\"animePopularity\"+\"\\t\"+\"animeDescription\"+\"\\t\"+\"animeRelated\"+\"\\t\"+\"animeCharacters\"+\"\\t\"+\"animeVoices\"+\"\\t\"+\"animeStaff\"+\"\\n\")\n",
    "    tsv.write(animeTitle+\"\\t\"+animeType+\"\\t\"+str(animeNumEpisode)+\"\\t\"+str(releaseDate)+\"\\t\"+str(endDate)+\"\\t\"+str(animeNumMembers)+\"\\t\"+str(animeScore)+\"\\t\"+str(animeUsers)+\"\\t\"+str(animeRank)+\"\\t\"+str(animePopularity)+\"\\t\"+animeDescription+\"\\t\"+str(animeRelated)+\"\\t\"+str(animeCharacters)+\"\\t\"+str(animeVoices)+\"\\t\"+str(animeStaff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in tqdm(range(0,382)):\n",
    "    \n",
    "    start=50*page\n",
    "    \n",
    "    for i in range(start,start+50):\n",
    "        #Open the articles of the page and save all the TSVs in a single directory\n",
    "        os.chdir(\"Page\"+str(page+1))\n",
    "        art=open(\"article\"+str(i+1)+'.html',\"r\")\n",
    "        os.chdir(\"..\")\n",
    "        os.chdir(\"TSV\")\n",
    "        tsv=open(\"anime_\"+str(i+1)+\".tsv\",\"w\")\n",
    "        \n",
    "        soup=BeautifulSoup(art.read(), \"html.parser\") \n",
    "        \n",
    "        #Just use the parsing function we developed earlier\n",
    "        Parse2(tsv,soup)\n",
    "        \n",
    "        art.close()\n",
    "        tsv.close()\n",
    "        os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XHYrcITLk1e"
   },
   "source": [
    "Creating a Dataframe to later insert every result into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "o1t-xQ3MP9TL"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df = pd.DataFrame(columns = ['animeTitle', 'animeType', 'animeNumEpisode', 'releaseDate', 'endDate', 'animeNumMembers', 'animeScore', \\\n",
    "           'animeUsers', 'animeRank', 'animePopularity', 'animeDescription', 'animeRelated', 'animeCharacters', \\\n",
    "           'animeVoices', 'animeStaff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibMiY6s1L6lc"
   },
   "source": [
    "Creating the function to fill the CSV file from the TSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "3rfnRrnuQRvh",
    "outputId": "3f657336-2bfb-4f38-93e4-09447591d6ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19082/19082 [02:45<00:00, 114.97it/s]\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/paolo/OneDrive/Desktop/TSV/anime_\"\n",
    "lista = range(12687,12701)\n",
    "def fillcsv(m,n):\n",
    "  for elem in tqdm(range(m,n)):\n",
    "    if elem not in lista:\n",
    "        tsv_file = open(path+str(elem)+\".tsv\",encoding=\"utf8\")\n",
    "        read_tsv = csv.reader(tsv_file, delimiter=\"\\t\")\n",
    "        cont = 0\n",
    "        for anime in read_tsv:\n",
    "        # using cont to avoid using the first line \n",
    "          if cont == 1:\n",
    "            anime = anime[0:15]\n",
    "            df.loc[elem] = anime\n",
    "          cont +=1\n",
    "fillcsv(1,19083)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vh7nuP-d_Hrn"
   },
   "source": [
    "#CREATING INDEX AND SEARCH ENGINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zX8ywQLLJgB"
   },
   "source": [
    "Here i removed the stopwords while creating the index for every word in the dataframe anime descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hkBE8bY0_HOT",
    "outputId": "377c40b5-3c02-4bfe-f8cc-96f882d8d4f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19067/19067 [03:33<00:00, 89.21it/s] \n"
     ]
    }
   ],
   "source": [
    "word_index = defaultdict(list)\n",
    "#n will run through the length of the dataframe\n",
    "n = df.shape[0]\n",
    "for i in tqdm(range(0,n)):\n",
    "  #fetching the anime descriptions\n",
    "  desc = df.iloc[i]['animeDescription']\n",
    "  splitted = desc.translate(str.maketrans('', '', string.punctuation)).split(\" \")\n",
    "  #creating the dictionary for the indices\n",
    "  for parola in splitted:\n",
    "    parola = parola.lower()\n",
    "    if parola not in stopwords.words('english'):\n",
    "      if parola not in word_index:\n",
    "        word_index[parola] = [[i]]\n",
    "      else:\n",
    "        if [i] not in word_index[parola]:\n",
    "          word_index[parola].append([i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcnvRdS_LSCa"
   },
   "source": [
    "Here i created 2 functions, one to fetch the URL of an anime and another one to unpack a list of lists as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReturnUrl(index):\n",
    "    f = open(\"C:/Users/paolo/OneDrive/Desktop/TSV/links.txt\",encoding=\"utf8\")\n",
    "    stringa = \"\"\n",
    "    for i, line in enumerate(f):\n",
    "        if i == index:\n",
    "            stringa = line\n",
    "    return stringa\n",
    "\n",
    "def listoflists(list):\n",
    "  flat_list = []\n",
    "  for elem in list:\n",
    "    flat_2 = []\n",
    "    for single in elem:\n",
    "      flat_2.append(single[0])\n",
    "    flat_list.append(flat_2)\n",
    "  return flat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i created another 2 functions, one is the search engine and the other one is used to display the results in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "WOBmDNTaGSHI"
   },
   "outputs": [],
   "source": [
    "def search_engine(query,dataset):\n",
    "  query = query.split(\" \")\n",
    "  query_indices = []\n",
    "  for index in query:\n",
    "    query_indices.append(word_index[index])\n",
    "  # used the set.intersection to see the items that are in both lists\n",
    "  query_indices = set.intersection(*map(set,listoflists(query_indices)))\n",
    "  query_index = set(query_indices)\n",
    "  res = list(query_index)\n",
    "  return(res)\n",
    "\n",
    "def displayresults(lista):\n",
    "    disp_res = pd.DataFrame()\n",
    "    disp_res = pd.DataFrame(columns = ['Anime Title','Anime Description','Anime URL'])\n",
    "    res = []\n",
    "    for i in lista:\n",
    "        roba = []\n",
    "        roba.append(df.iloc[i]['animeTitle'])\n",
    "        roba.append(df.iloc[i]['animeDescription'])\n",
    "        roba.append(ReturnUrl(i))\n",
    "        res.append(roba)\n",
    "    index = 0\n",
    "    for elem in res:\n",
    "        disp_res.loc[index] = elem\n",
    "        index += 1\n",
    "    return(display(disp_res))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_rbUj7PLaKS"
   },
   "source": [
    "Testing the query for a random input from the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OwufJsPvALpG",
    "outputId": "72f09e4c-2632-49a4-f667-7cb2f51681ef",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your anime features: saiyan race\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anime Title</th>\n",
       "      <th>Anime Description</th>\n",
       "      <th>Anime URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dragon Ball Super: Broly</td>\n",
       "      <td>Forty-one years ago on Planet Vegeta, home of ...</td>\n",
       "      <td>https://myanimelist.net/anime/36946/Dragon_Bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "      <td>Bardock, Son Goku's father, is a low-ranking S...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dragon Ball Z</td>\n",
       "      <td>Five years after winning the World Martial Art...</td>\n",
       "      <td>https://myanimelist.net/anime/813/Dragon_Ball_Z\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Anime Title  \\\n",
       "0                           Dragon Ball Super: Broly   \n",
       "1  Dragon Ball Z Special 1: Tatta Hitori no Saish...   \n",
       "2                                      Dragon Ball Z   \n",
       "\n",
       "                                   Anime Description  \\\n",
       "0  Forty-one years ago on Planet Vegeta, home of ...   \n",
       "1  Bardock, Son Goku's father, is a low-ranking S...   \n",
       "2  Five years after winning the World Martial Art...   \n",
       "\n",
       "                                           Anime URL  \n",
       "0  https://myanimelist.net/anime/36946/Dragon_Bal...  \n",
       "1  https://myanimelist.net/anime/986/Dragon_Ball_...  \n",
       "2  https://myanimelist.net/anime/813/Dragon_Ball_Z\\n  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = str(input(\"Insert your anime features: \"))\n",
    "displayresults(search_engine(query,df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2.2 CONJUNCTIVE QUERY & RANKING SCORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i created the inverse index dictionary for every word, it will append the term frequency of the word and create the idf for a certain word in the dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverse(n,df):\n",
    "    inverse_index = {}\n",
    "    for i in tqdm(range(0,n)):\n",
    "      #fetching the anime descriptions\n",
    "      desc = df.iloc[i]['animeDescription']\n",
    "      desc = desc.lower()\n",
    "      splitted = desc.translate(str.maketrans('', '', string.punctuation)).split(\" \")\n",
    "      #creating the dictionary for the indices\n",
    "      for parola in splitted:\n",
    "        parola = parola.lower()\n",
    "        occurrences = splitted.count(parola)\n",
    "        if parola not in stopwords.words('english'):\n",
    "          if parola not in inverse_index:\n",
    "            # If the word was never encountered we add it and its tf\n",
    "            inverse_index[parola] = [[i,round(occurrences/len(splitted),4)]]\n",
    "          else:\n",
    "            # if the  word was encountered we simply check first that it-s not a duplicate (2 words in the same doc)\n",
    "            if [i,round((occurrences/len(splitted)),4)] not in inverse_index[parola]:\n",
    "                # if it was not a duplicate then we add the document and the new tf to the work values\n",
    "              inverse_index[parola].append([i,round(occurrences/len(splitted),4)])\n",
    "    # then we just calculate the idf \n",
    "    for elem in inverse_index:\n",
    "        inverse_index[elem].append(round(math.log(df.shape[0]/len(inverse_index[elem])),4))\n",
    "    return inverse_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19067/19067 [03:39<00:00, 86.97it/s] \n"
     ]
    }
   ],
   "source": [
    "res = create_inverse(df.shape[0],df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTION TO CREATE THE TF_IDF FROM THE WORD_INDEX, just a simple multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(index):\n",
    "    for word in index:\n",
    "        for elem in index[word]:\n",
    "            df = index[word][-1]\n",
    "            if isinstance(elem, list):\n",
    "                tfxidf = round(elem[1]*df,3)\n",
    "                elem[1] = tfxidf\n",
    "    return index\n",
    "tf_idf = calculate_tfidf(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just calculate the tf_idf for the query terms to calculate the cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termFrequency(term, document):\n",
    "    normalizeDocument = document.lower().split()\n",
    "    return normalizeDocument.count(term.lower()) / float(len(normalizeDocument))\n",
    "def compute_query_tf(query):\n",
    "    query_norm_tf = {}\n",
    "    tokens = query.split()\n",
    "    for word in tokens:\n",
    "        query_norm_tf[word] = termFrequency(word , query)\n",
    "    return query_norm_tf\n",
    "def compute_query_tfidf(query):\n",
    "    tf_idf_query = []\n",
    "    for elem in query:\n",
    "        tf_idf_query.append([elem,query[elem]*res[elem][-1]])\n",
    "    return tf_idf_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the search engine to fetch the results to calculate the cosine similarity on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine_2(query,dataset):\n",
    "  query = query.split(\" \")\n",
    "  query_indices = []\n",
    "  for term in query:\n",
    "    if term not in stopwords.words('english'):\n",
    "        query_indices.append(res[term])\n",
    "  docs_tfidf = defaultdict(list)\n",
    "  for match in query_indices:\n",
    "        for document in match:\n",
    "            if isinstance(document, list):\n",
    "                docs_tfidf[document[0]].append(document[1])\n",
    "  docs_tfidf2 = defaultdict(list)\n",
    "  for elem in docs_tfidf:\n",
    "    if len(docs_tfidf[elem])>(len(query_indices)-1):\n",
    "        docs_tfidf2[elem].append(docs_tfidf[elem])\n",
    "  return docs_tfidf2\n",
    "\n",
    "def displayresults(lista):\n",
    "    disp_res = pd.DataFrame()\n",
    "    disp_res = pd.DataFrame(columns = ['Anime Title','Anime Description','Anime URL','Score'])\n",
    "    res = []\n",
    "    for i in lista:\n",
    "        roba = []\n",
    "        roba.append(df.iloc[i[0]]['animeTitle'])\n",
    "        roba.append(df.iloc[i[0]]['animeDescription'])\n",
    "        roba.append(ReturnUrl(i[0]))\n",
    "        roba.append(i[1])\n",
    "        res.append(roba)\n",
    "    index = 0\n",
    "    for elem in res:\n",
    "        disp_res.loc[index] = elem\n",
    "        index += 1\n",
    "    return(display(disp_res))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dragon Ball Z'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[364][\"animeTitle\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving an input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your anime features: fighting magic\n"
     ]
    }
   ],
   "source": [
    "words = str(input(\"Insert your anime features: \"))\n",
    "index = search_engine_2(words,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anime Title</th>\n",
       "      <th>Anime Description</th>\n",
       "      <th>Anime URL</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Game No Life: Zero</td>\n",
       "      <td>In ancient Disboard, Riku is an angry, young w...</td>\n",
       "      <td>https://myanimelist.net/anime/33674/No_Game_No...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kekkai Sensen &amp; Beyond</td>\n",
       "      <td>Three years ago, a gateway between Earth and t...</td>\n",
       "      <td>https://myanimelist.net/anime/34451/Kekkai_Sen...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kore wa Zombie Desu ka?</td>\n",
       "      <td>Ayumu Aikawa is a 16-year-old high school stud...</td>\n",
       "      <td>https://myanimelist.net/anime/8841/Kore_wa_Zom...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mahou Sensei Negima!</td>\n",
       "      <td>10-year-old Negi Springfield is a wizard-in-tr...</td>\n",
       "      <td>https://myanimelist.net/anime/157/Mahou_Sensei...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Silent Möbius: The Motion Picture</td>\n",
       "      <td>In a futuristic Tokyo, several policewomen fig...</td>\n",
       "      <td>https://myanimelist.net/anime/2099/Silent_Möbi...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aquarian Age: Saga II - Don't Forget Me...</td>\n",
       "      <td>During the chaotic period known in Western ast...</td>\n",
       "      <td>https://myanimelist.net/anime/1268/Aquarian_Ag...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gakuentoshi Varanoir Joukan</td>\n",
       "      <td>Based on a Playstation 2 game by the same titl...</td>\n",
       "      <td>https://myanimelist.net/anime/3533/Gakuentoshi...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Generation of Chaos III: Toki no Fuuin</td>\n",
       "      <td>Fuujin Year 130 (Medieval times when magic and...</td>\n",
       "      <td>https://myanimelist.net/anime/2778/Generation_...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Zhu Zhu Xia: Yong Chuang Juren Dao</td>\n",
       "      <td>Jack the Robber loses his magic bean. G.G. Bon...</td>\n",
       "      <td>https://myanimelist.net/anime/38117/Zhu_Zhu_Xi...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mahou no Tenshi Creamy Mami</td>\n",
       "      <td>Creamy Mami is about a young girl, Yuu, who af...</td>\n",
       "      <td>https://myanimelist.net/anime/2044/Mahou_no_Te...</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ninkuu (Movie)</td>\n",
       "      <td>How much humiliation can four people and a pen...</td>\n",
       "      <td>https://myanimelist.net/anime/2851/Ninkuu_Movie\\n</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ojamajo Doremi</td>\n",
       "      <td>Harukaze Doremi considers herself to be the un...</td>\n",
       "      <td>https://myanimelist.net/anime/350/Ojamajo_Dore...</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mahou Sensou</td>\n",
       "      <td>The world as we know it is actually just half ...</td>\n",
       "      <td>https://myanimelist.net/anime/19769/Mahou_Sens...</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Anime Title  \\\n",
       "0                        No Game No Life: Zero   \n",
       "1                       Kekkai Sensen & Beyond   \n",
       "2                      Kore wa Zombie Desu ka?   \n",
       "3                         Mahou Sensei Negima!   \n",
       "4            Silent Möbius: The Motion Picture   \n",
       "5   Aquarian Age: Saga II - Don't Forget Me...   \n",
       "6                  Gakuentoshi Varanoir Joukan   \n",
       "7       Generation of Chaos III: Toki no Fuuin   \n",
       "8           Zhu Zhu Xia: Yong Chuang Juren Dao   \n",
       "9                  Mahou no Tenshi Creamy Mami   \n",
       "10                              Ninkuu (Movie)   \n",
       "11                              Ojamajo Doremi   \n",
       "12                                Mahou Sensou   \n",
       "\n",
       "                                    Anime Description  \\\n",
       "0   In ancient Disboard, Riku is an angry, young w...   \n",
       "1   Three years ago, a gateway between Earth and t...   \n",
       "2   Ayumu Aikawa is a 16-year-old high school stud...   \n",
       "3   10-year-old Negi Springfield is a wizard-in-tr...   \n",
       "4   In a futuristic Tokyo, several policewomen fig...   \n",
       "5   During the chaotic period known in Western ast...   \n",
       "6   Based on a Playstation 2 game by the same titl...   \n",
       "7   Fuujin Year 130 (Medieval times when magic and...   \n",
       "8   Jack the Robber loses his magic bean. G.G. Bon...   \n",
       "9   Creamy Mami is about a young girl, Yuu, who af...   \n",
       "10  How much humiliation can four people and a pen...   \n",
       "11  Harukaze Doremi considers herself to be the un...   \n",
       "12  The world as we know it is actually just half ...   \n",
       "\n",
       "                                            Anime URL  Score  \n",
       "0   https://myanimelist.net/anime/33674/No_Game_No...   1.00  \n",
       "1   https://myanimelist.net/anime/34451/Kekkai_Sen...   1.00  \n",
       "2   https://myanimelist.net/anime/8841/Kore_wa_Zom...   1.00  \n",
       "3   https://myanimelist.net/anime/157/Mahou_Sensei...   1.00  \n",
       "4   https://myanimelist.net/anime/2099/Silent_Möbi...   1.00  \n",
       "5   https://myanimelist.net/anime/1268/Aquarian_Ag...   1.00  \n",
       "6   https://myanimelist.net/anime/3533/Gakuentoshi...   1.00  \n",
       "7   https://myanimelist.net/anime/2778/Generation_...   1.00  \n",
       "8   https://myanimelist.net/anime/38117/Zhu_Zhu_Xi...   1.00  \n",
       "9   https://myanimelist.net/anime/2044/Mahou_no_Te...   0.95  \n",
       "10  https://myanimelist.net/anime/2851/Ninkuu_Movie\\n   0.95  \n",
       "11  https://myanimelist.net/anime/350/Ojamajo_Dore...   0.89  \n",
       "12  https://myanimelist.net/anime/19769/Mahou_Sens...   0.89  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_idf = compute_query_tfidf(compute_query_tf(words))\n",
    "final_query = []\n",
    "for elem in query_idf:\n",
    "    final_query.append(elem[1])\n",
    "import numpy as np\n",
    "def cosine_similarity(query,matches):\n",
    "    matches_scores = {}\n",
    "    for elem in matches:\n",
    "        doc = matches[elem][0]\n",
    "        matches_scores[elem] = round(np.dot(query,doc)/(np.linalg.norm(query)*np.linalg.norm(doc)),2)\n",
    "    return(sorted(matches_scores.items(), key=lambda matches_scores: matches_scores[1],reverse = True))\n",
    "final_result = (cosine_similarity(final_query,index))\n",
    "displayresults(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the user to give importance to more than just the description, we created a cosine similarity for the topics but also weighted it for the episodes and the anime popularity, this leads to more than just giving back the animes that talk about "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How important is the episode number from 1 to 10?: 10\n",
      "How many episodes would you like to watch?: 35\n"
     ]
    }
   ],
   "source": [
    "for elem in range(0,100):\n",
    "    episodes_importance = int(input(\"How important is the episode number from 1 to 10?: \"))\n",
    "    desired_episodes = int(input(\"How many episodes would you like to watch?: \"))\n",
    "    if episodes_importance <= 10 and episodes_importance >0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How important is the popularity from 1 to 10? 10\n",
      "What rank of popularity from 1 to 10?: 3\n"
     ]
    }
   ],
   "source": [
    "for elem in range(0,100):\n",
    "    pop_imp = int(input(\"How important is the popularity from 1 to 10? \"))\n",
    "    if pop_imp <= 10 and pop_imp >0:\n",
    "        break\n",
    "popularity_ranks = []\n",
    "for elem in range(0,19067,1906):\n",
    "    popularity_ranks.append(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What would you like the anime to talk about? 3 word max kill man\n"
     ]
    }
   ],
   "source": [
    "for elem in range(0,10000):\n",
    "    topics = str(input(\"What would you like the anime to talk about? 3 word max \"))\n",
    "    query = topics.split(\" \")\n",
    "    if len(query) >1 and  len(query) <4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = search_engine_2(topics,df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create the final table to upload the results into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(query,cosine,episodes,popularity):\n",
    "    disp_res = pd.DataFrame()\n",
    "    disp_res = pd.DataFrame(columns = ['Anime Title','Anime URL','cosine_core','episode_score','popularity_score','final_score'])\n",
    "    res = []\n",
    "    for i in range(0,len(query)):\n",
    "        roba = []\n",
    "        roba.append(df.iloc[query[i][0]]['animeTitle'])\n",
    "        roba.append(ReturnUrl(query[i][0]))\n",
    "        roba.append(cosine[i])\n",
    "        roba.append(final_score_episodes[i])\n",
    "        roba.append(final_popularity_score[i])\n",
    "        roba.append((cosine[i]+final_score_episodes[i]*episodes_importance+final_popularity_score[i]*pop_imp)/(1+1*episodes_importance+1*pop_imp))\n",
    "        res.append(roba)\n",
    "    index = 0\n",
    "    for elem in res:\n",
    "        disp_res.loc[index] = elem\n",
    "        index += 1\n",
    "    return(disp_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate the cosine similarity for the TF_IDFS§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_idf = compute_query_tfidf(compute_query_tf(topics))\n",
    "final_query = []\n",
    "for elem in query_idf:\n",
    "    final_query.append(elem[1])\n",
    "import numpy as np\n",
    "def cosine_similarity(query,matches):\n",
    "    matches_scores = {}\n",
    "    for elem in matches:\n",
    "        doc = matches[elem][0]\n",
    "        matches_scores[elem] = round(np.dot(query,doc)/(np.linalg.norm(query)*np.linalg.norm(doc)),2)\n",
    "    return(sorted(matches_scores.items(), key=lambda matches_scores: matches_scores[1],reverse = True))\n",
    "final_result = (cosine_similarity(final_query,index))\n",
    "cosine_scores = []\n",
    "for elem in final_result:\n",
    "    cosine_scores.append(elem[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Episode Score Contribution, for this part i defined 2 lists of 20 elements to calculate on a scale from 1 to 10 how our actual episodes score in reference to the user likings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score_episodes = []\n",
    "for elem in final_result:\n",
    "    actual_episodes = int(df.iloc[elem[0]][\"animeNumEpisode\"])\n",
    "    episode_lower_distribution = []\n",
    "    for elem in range(0,desired_episodes, desired_episodes//10):\n",
    "        episode_lower_distribution.append(elem)\n",
    "    episode_upper_distribution = []\n",
    "    for elem in range(desired_episodes, desired_episodes*2, desired_episodes//10):\n",
    "        episode_upper_distribution.append(elem)\n",
    "    cont = 10\n",
    "    if actual_episodes >= episode_upper_distribution[-1]:\n",
    "        final_score_episodes.append(0.1)\n",
    "    else:\n",
    "        if actual_episodes == episode_lower_distribution[-1]:\n",
    "            final_score_episodes.append(0.9)\n",
    "        for score in range(1,10):\n",
    "            if actual_episodes > episode_lower_distribution[-1]:\n",
    "                cont -= 1\n",
    "                if actual_episodes >= episode_upper_distribution[score-1] and actual_episodes< episode_upper_distribution[score] :\n",
    "                    final_score_episodes.append((cont/10))\n",
    "            else:\n",
    "                if actual_episodes >= episode_lower_distribution[score-1] and actual_episodes < episode_lower_distribution[score] :\n",
    "                    final_score_episodes.append((score/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part instead i defined a list of 10 popularity ranks to see how our anime popularity scores in respect to the user likings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_popularity_score = []\n",
    "for elem in final_result:\n",
    "    anime = elem[0]\n",
    "    actual_pop = int(df.iloc[anime][\"animePopularity\"])\n",
    "    score = 0\n",
    "    for i in range(1,len(popularity_ranks)):\n",
    "        if actual_pop >= popularity_ranks[i-1] and actual_pop < popularity_ranks[i]:\n",
    "            final_popularity_score.append(i/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i simply fetch the results and put them in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anime Title</th>\n",
       "      <th>Anime URL</th>\n",
       "      <th>cosine_core</th>\n",
       "      <th>episode_score</th>\n",
       "      <th>popularity_score</th>\n",
       "      <th>final_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ninpuu Kamui Gaiden</td>\n",
       "      <td>https://myanimelist.net/anime/5023/Ninpuu_Kamu...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ayashi no Ceres</td>\n",
       "      <td>https://myanimelist.net/anime/104/Ayashi_no_Ce...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mugen no Juunin: Immortal</td>\n",
       "      <td>https://myanimelist.net/anime/39806/Mugen_no_J...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Gun x Sword</td>\n",
       "      <td>https://myanimelist.net/anime/411/Gun_x_Sword\\n</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.561905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Death Note</td>\n",
       "      <td>https://myanimelist.net/anime/1535/Death_Note\\n</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.523810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vinland Saga</td>\n",
       "      <td>https://myanimelist.net/anime/37521/Vinland_Sa...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.523810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Koroshiya-san: The Hired Gun</td>\n",
       "      <td>https://myanimelist.net/anime/20329/Koroshiya-...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Legend of Duo</td>\n",
       "      <td>https://myanimelist.net/anime/600/Legend_of_Duo\\n</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Shin Karate Jigoku-hen</td>\n",
       "      <td>https://myanimelist.net/anime/20963/Shimajirou...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Blade of the Immortal</td>\n",
       "      <td>https://myanimelist.net/anime/4151/Blade_of_th...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Minerva no Kenshi</td>\n",
       "      <td>https://myanimelist.net/anime/4946/Angel_Blade...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Animatrix</td>\n",
       "      <td>https://myanimelist.net/anime/1303/The_Animatr...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Highschool of the Dead</td>\n",
       "      <td>https://myanimelist.net/anime/8074/Highschool_...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chu Feng: B.E.E</td>\n",
       "      <td>https://myanimelist.net/anime/29876/Chu_Feng__...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Seikoujo: Haitoku no Biden Dorei</td>\n",
       "      <td>https://myanimelist.net/anime/10938/Innocent__...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Harukanaru Toki no Naka de: Maihitoyo</td>\n",
       "      <td>https://myanimelist.net/anime/1906/Harukanaru_...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Muramasa</td>\n",
       "      <td>https://myanimelist.net/anime/3475/Muramasa\\n</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Midnight Eye: Gokuu II</td>\n",
       "      <td>https://myanimelist.net/anime/1880/Midnight_Ey...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Shin Hokuto no Ken</td>\n",
       "      <td>https://myanimelist.net/anime/1357/Shin_Hokuto...</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.279048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hokuto no Ken: Raoh Gaiden Junai-hen</td>\n",
       "      <td>https://myanimelist.net/anime/1773/Hokuto_no_K...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.238095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Rurouni Kenshin: Meiji Kenkaku Romantan - Tsui...</td>\n",
       "      <td>https://myanimelist.net/anime/44/Rurouni_Kensh...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.188095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shingeki no Kyojin: Kuinaki Sentaku</td>\n",
       "      <td>https://myanimelist.net/anime/25781/Shingeki_n...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Anime Title  \\\n",
       "10                                Ninpuu Kamui Gaiden   \n",
       "6                                     Ayashi no Ceres   \n",
       "4                           Mugen no Juunin: Immortal   \n",
       "21                                        Gun x Sword   \n",
       "1                                          Death Note   \n",
       "0                                        Vinland Saga   \n",
       "14                       Koroshiya-san: The Hired Gun   \n",
       "15                                      Legend of Duo   \n",
       "16                             Shin Karate Jigoku-hen   \n",
       "9                               Blade of the Immortal   \n",
       "17                                  Minerva no Kenshi   \n",
       "5                                       The Animatrix   \n",
       "8                              Highschool of the Dead   \n",
       "13                                    Chu Feng: B.E.E   \n",
       "18                   Seikoujo: Haitoku no Biden Dorei   \n",
       "7               Harukanaru Toki no Naka de: Maihitoyo   \n",
       "12                                           Muramasa   \n",
       "11                             Midnight Eye: Gokuu II   \n",
       "20                                 Shin Hokuto no Ken   \n",
       "3                Hokuto no Ken: Raoh Gaiden Junai-hen   \n",
       "19  Rurouni Kenshin: Meiji Kenkaku Romantan - Tsui...   \n",
       "2                 Shingeki no Kyojin: Kuinaki Sentaku   \n",
       "\n",
       "                                            Anime URL  cosine_core  \\\n",
       "10  https://myanimelist.net/anime/5023/Ninpuu_Kamu...         1.00   \n",
       "6   https://myanimelist.net/anime/104/Ayashi_no_Ce...         1.00   \n",
       "4   https://myanimelist.net/anime/39806/Mugen_no_J...         1.00   \n",
       "21    https://myanimelist.net/anime/411/Gun_x_Sword\\n         0.80   \n",
       "1     https://myanimelist.net/anime/1535/Death_Note\\n         1.00   \n",
       "0   https://myanimelist.net/anime/37521/Vinland_Sa...         1.00   \n",
       "14  https://myanimelist.net/anime/20329/Koroshiya-...         1.00   \n",
       "15  https://myanimelist.net/anime/600/Legend_of_Duo\\n         1.00   \n",
       "16  https://myanimelist.net/anime/20963/Shimajirou...         1.00   \n",
       "9   https://myanimelist.net/anime/4151/Blade_of_th...         1.00   \n",
       "17  https://myanimelist.net/anime/4946/Angel_Blade...         1.00   \n",
       "5   https://myanimelist.net/anime/1303/The_Animatr...         1.00   \n",
       "8   https://myanimelist.net/anime/8074/Highschool_...         1.00   \n",
       "13  https://myanimelist.net/anime/29876/Chu_Feng__...         1.00   \n",
       "18  https://myanimelist.net/anime/10938/Innocent__...         1.00   \n",
       "7   https://myanimelist.net/anime/1906/Harukanaru_...         1.00   \n",
       "12      https://myanimelist.net/anime/3475/Muramasa\\n         1.00   \n",
       "11  https://myanimelist.net/anime/1880/Midnight_Ey...         1.00   \n",
       "20  https://myanimelist.net/anime/1357/Shin_Hokuto...         0.86   \n",
       "3   https://myanimelist.net/anime/1773/Hokuto_no_K...         1.00   \n",
       "19  https://myanimelist.net/anime/44/Rurouni_Kensh...         0.95   \n",
       "2   https://myanimelist.net/anime/25781/Shingeki_n...         1.00   \n",
       "\n",
       "    episode_score  popularity_score  final_score  \n",
       "10            0.9               0.6     0.761905  \n",
       "6             0.9               0.2     0.571429  \n",
       "4             0.9               0.2     0.571429  \n",
       "21            0.9               0.2     0.561905  \n",
       "1             0.9               0.1     0.523810  \n",
       "0             0.9               0.1     0.523810  \n",
       "14            0.4               0.4     0.428571  \n",
       "15            0.5               0.3     0.428571  \n",
       "16            0.1               0.7     0.428571  \n",
       "9             0.5               0.2     0.380952  \n",
       "17            0.2               0.5     0.380952  \n",
       "5             0.4               0.2     0.333333  \n",
       "8             0.5               0.1     0.333333  \n",
       "13            0.3               0.3     0.333333  \n",
       "18            0.1               0.5     0.333333  \n",
       "7             0.1               0.4     0.285714  \n",
       "12            0.1               0.4     0.285714  \n",
       "11            0.1               0.4     0.285714  \n",
       "20            0.2               0.3     0.279048  \n",
       "3             0.1               0.3     0.238095  \n",
       "19            0.2               0.1     0.188095  \n",
       "2             0.1               0.1     0.142857  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = display_results(final_result,cosine_scores,final_score_episodes,final_popularity_score)\n",
    "result.sort_values(by=[\"final_score\"],ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for particular tags to get the reviews\n",
    "def saveReviews(rev,soup):\n",
    "    revs=soup.find_all(\"div\",{\"class\":\"spaceit textReadability word-break pt8 mt8\",\n",
    "                     \"style\":\"clear: both; border-top: 1px solid #ebebeb;\"})\n",
    "    if len(revs)>1:\n",
    "        for i in range(len(revs)):\n",
    "            single_rev=revs[i].getText().strip().replace(\"\\n\",\"\")[80:580]\n",
    "            rev.write(single_rev+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the reviews\n",
    "for page in tqdm(range(0,382)):\n",
    "\n",
    "    start=50*page\n",
    "\n",
    "    for i in range(start,start+50):\n",
    "        #Open the articles of the page and save all the revies in a single directory\n",
    "        os.chdir(\"Page\"+str(page+1))\n",
    "        art=open(\"article\"+str(i+1)+'.html',\"r\")\n",
    "        os.chdir(\"..\")\n",
    "        os.chdir(\"Reviews\")\n",
    "        rev=open(\"review_\"+str(i+1)+\".txt\",\"w\")\n",
    "\n",
    "        soup=BeautifulSoup(art.read(), \"html.parser\") \n",
    "\n",
    "        #Just use the saving function we developed earlier\n",
    "        saveReviews(rev,soup)\n",
    "\n",
    "        art.close()\n",
    "        rev.close()\n",
    "        os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that applies tokenize,lower,removes stopwords and stemming to a given string\n",
    "def clean(string):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    string=tokenizer.tokenize(string)\n",
    "    for word in range(len(string)):\n",
    "        string[word] = string[word].lower() \n",
    "    #Remove stopwords\n",
    "    string = [word for word in string if not word in stopwords.words()]\n",
    "    #STEMMING\n",
    "    stemmer = PorterStemmer()\n",
    "    string = [stemmer.stem(word) for word in string]\n",
    "    return str(\" \".join(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_comment=[]\n",
    "os.chdir(\"Reviews\")\n",
    "comments=[]\n",
    "art_comment=[]\n",
    "for page in tqdm(range(0,19083)):\n",
    "    if os.stat(\"review_\"+str(page+1)+\".txt\").st_size != 0: #do all the stuff below if the file is bigger than 0 bytes,\n",
    "                                                           #so just do it if it's not empty\n",
    "        rev=open(\"review_\"+str(page+1)+\".txt\",\"r\")\n",
    "        lines=rev.readlines()\n",
    "        for i in range(len(lines)):\n",
    "            art_comment.append(clean(lines[i]))\n",
    "        comments.append(str(art_comment)+\"\\n\")            #we are saving as a list of lists the \"cleaned\" comments for \n",
    "                                                          #every page like so\n",
    "                                                          #[[comment1page1,comment2page1,..][comment1page2,..]...]\n",
    "        art_comment=[]\n",
    "        rev.close()\n",
    "    else:\n",
    "        comments.append(\"\\t\\n\")\n",
    "    \n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results\n",
    "os.chdir(\"Reviews\")\n",
    "c=open(\"comments.txt\",\"w\")\n",
    "for comment in comments:\n",
    "    c.write(str(comment))\n",
    "c.close()\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load results\n",
    "comments=[]\n",
    "c=open(\"comments.txt\",\"r\")\n",
    "lines=c.readlines()\n",
    "for i in range(len(lines)):\n",
    "    comments.append(lines[i].replace(\"\\n\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",'').split(\",\"))\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19083/19083 [00:00<00:00, 86610.34it/s]\n"
     ]
    }
   ],
   "source": [
    "#build corpus with the single words appearing in all reviews\n",
    "corpus={}\n",
    "#update=1\n",
    "for i in tqdm(range(len(comments))):\n",
    "    #print(\"i={}\".format(i))\n",
    "    for j in range(len(comments[i])):\n",
    "        sentence=comments[i][j].split(\" \")\n",
    "        \n",
    "        for word in sentence:\n",
    "            \n",
    "            if word not in corpus:\n",
    "                corpus.update({word:0})\n",
    "                #update=update+1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19083/19083 [01:19<00:00, 239.03it/s]\n"
     ]
    }
   ],
   "source": [
    "#build vector with the count of words appearing in all reviews\n",
    "vectors=[]\n",
    "vec_sentence=[]\n",
    "doc=dict.fromkeys(corpus, 0)\n",
    "for i in tqdm(range(0,len(comments))):\n",
    "    vec_sentence=[]\n",
    "    for j in range(len(comments[i])):\n",
    "        \n",
    "        sentence=comments[i][j].split(\" \")\n",
    "        for word in sentence:\n",
    "            doc[word] += 1\n",
    "        \n",
    "        vec_sentence.append(list(doc.values()))\n",
    "        doc=dict.fromkeys(corpus, 0)\n",
    "    vectors.append(vec_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36595\n",
      "19083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19083"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dimensions are fine\n",
    "print(len(corpus))\n",
    "print(len(comments[:][:]))\n",
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "for i in range(len(vectors)):\n",
    "    for j in range(len(vectors[i])):\n",
    "        X.append(vectors[i][j])\n",
    "X=np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#free some space on memory\n",
    "def release_list(a):\n",
    "    del a[:]\n",
    "    del a\n",
    "release_list(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(init=\"random\", n_clusters=2, n_init=30, max_iter=5000).fit(X)\n",
    "y=(kmeans.labels_)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13192 23493\n"
     ]
    }
   ],
   "source": [
    "num_zeros = (y == 0).sum()\n",
    "num_ones = (y == 1).sum()\n",
    "print(num_zeros,num_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are applying KMeans (k=2) to vectors belonging to $R^{36595}$. The results of the clustering clearly show a realistic result, but it's hard to quantify how accurate the result is.\\\n",
    "Probably a better way to solve the problem would be implementing a neural network, but it would take a lot of time to assign the labels to the comments (those are needed for the training). Or maybe it could also work to train the neural network with the results obtained from the clustering, but we can't estimate how accurate the KMeans is and the error related to this would carry over the neural network itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 5 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pseudocode:\n",
    "\"\"\"\n",
    "A: array of length l of which we want to compute the maximum sum of non adjacent elements\n",
    "\n",
    "def compute(A): \n",
    "    l=len(A)       \n",
    "    gain=[]      declare 2 arrays: one to store the values of the gain at each iteration and the other \n",
    "    path=[]                        to use later to save only the elements of the gain we care about (the path chosen\n",
    "                                   to get the sum)\n",
    "    inc=exc=0    here we initialize the variables that will keep inside them the value of the 2 sums at each iteration:\n",
    "                 one including and one excluding the single element \n",
    "                 \n",
    "    for i in range(l):          then we just compute the value of the 2 sums at each iteration and save the gain \n",
    "        temp=inc                for each of them\n",
    "        inc=max(inc,A[i]+exc)\n",
    "        exc=temp            \n",
    "        gain[i]=(inc-exc)\n",
    "    \n",
    "    for j in range(0,len(gain)-1,2):  \n",
    "        if gain[j+1]>0:               here we compare the elements of the gain list and take only those of interest:\n",
    "            path[j]=(A[j+1])          if the gain is equal to 0 for that iteration the value is excluded \"a priori\"\n",
    "        else:                         then we impose the condition to only get alternate values for which the gain is\n",
    "            path[j]=(A[j])            greater than zero\n",
    "    \n",
    "    return sum(path),path\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation\n",
    "def compute(A):\n",
    "    l=len(A)\n",
    "    gain=[]\n",
    "    path=[]\n",
    "    inc=exc=0 #include, exclude --> compute value of sum including or excluding the i-th element\n",
    "\n",
    "    for i in range(l):\n",
    "        temp=inc\n",
    "        inc=max(inc,A[i]+exc)\n",
    "        exc=temp            \n",
    "        gain.append(inc-exc)#save the gain\n",
    "\n",
    "    \n",
    "    for j in range(0,len(gain)-1,2):\n",
    "        if gain[j+1]>0:\n",
    "            path.append(A[j+1])\n",
    "        else:\n",
    "            path.append(A[j])\n",
    "    \n",
    "    return sum(path),path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an array A of length $n$ the complexity of the algorithm is $O(n)$ [excluding constants].\\\n",
    "It took me over 8 hours to come up with this solution (I had problems with the implementation), in which I tried different approaches to the problem:\n",
    "\n",
    "1. I first tried looking at the list as a tree, but computing every possible case to then take the one with the highest output would lead me to a solution of order $O(n^2)$.\n",
    "1. After this attempt I started searching on the internet and came across this video (https://www.youtube.com/watch?v=UtGtF6nc35g) in which the inclusion/exclusion method is explained. Making few additions to that algorithm I managed to get a result that is able to find the highest value solution along with the chosen elements of the starting array that seems to work in all the cases and in $O(n)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "ADM_HW3.ipynb",
   "provenance": []
  },
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
