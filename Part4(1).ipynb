{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#look for particular tags to get the reviews\n",
    "def saveReviews(rev,soup):\n",
    "    revs=soup.find_all(\"div\",{\"class\":\"spaceit textReadability word-break pt8 mt8\",\n",
    "                     \"style\":\"clear: both; border-top: 1px solid #ebebeb;\"})\n",
    "    if len(revs)>1:\n",
    "        for i in range(len(revs)):\n",
    "            single_rev=revs[i].getText().strip().replace(\"\\n\",\"\")[80:580]\n",
    "            rev.write(single_rev+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the reviews\n",
    "for page in tqdm(range(0,382)):\n",
    "\n",
    "    start=50*page\n",
    "\n",
    "    for i in range(start,start+50):\n",
    "        #Open the articles of the page and save all the revies in a single directory\n",
    "        os.chdir(\"Page\"+str(page+1))\n",
    "        art=open(\"article\"+str(i+1)+'.html',\"r\")\n",
    "        os.chdir(\"..\")\n",
    "        os.chdir(\"Reviews\")\n",
    "        rev=open(\"review_\"+str(i+1)+\".txt\",\"w\")\n",
    "\n",
    "        soup=BeautifulSoup(art.read(), \"html.parser\") \n",
    "\n",
    "        #Just use the saving function we developed earlier\n",
    "        saveReviews(rev,soup)\n",
    "\n",
    "        art.close()\n",
    "        rev.close()\n",
    "        os.chdir(\"..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that applies tokenize,lower,removes stopwords and stemming to a given string\n",
    "def clean(string):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    string=tokenizer.tokenize(string)\n",
    "    for word in range(len(string)):\n",
    "        string[word] = string[word].lower() \n",
    "    #Remove stopwords\n",
    "    string = [word for word in string if not word in stopwords.words()]\n",
    "    #STEMMING\n",
    "    stemmer = PorterStemmer()\n",
    "    string = [stemmer.stem(word) for word in string]\n",
    "    return str(\" \".join(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "art_comment=[]\n",
    "os.chdir(\"Reviews\")\n",
    "comments=[]\n",
    "art_comment=[]\n",
    "for page in tqdm(range(0,19083)):\n",
    "    if os.stat(\"review_\"+str(page+1)+\".txt\").st_size != 0: #do all the stuff below if the file is bigger than 0 bytes,\n",
    "                                                           #so just do it if it's not empty\n",
    "        rev=open(\"review_\"+str(page+1)+\".txt\",\"r\")\n",
    "        lines=rev.readlines()\n",
    "        for i in range(len(lines)):\n",
    "            art_comment.append(clean(lines[i]))\n",
    "        comments.append(str(art_comment)+\"\\n\")            #we are saving as a list of lists the \"cleaned\" comments for \n",
    "                                                          #every page like so\n",
    "                                                          #[[comment1page1,comment2page1,..][comment1page2,..]...]\n",
    "        art_comment=[]\n",
    "        rev.close()\n",
    "    else:\n",
    "        comments.append(\"\\t\\n\")\n",
    "    \n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results\n",
    "os.chdir(\"Reviews\")\n",
    "c=open(\"comments.txt\",\"w\")\n",
    "for comment in comments:\n",
    "    c.write(str(comment))\n",
    "c.close()\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load results\n",
    "comments=[]\n",
    "c=open(\"comments.txt\",\"r\")\n",
    "lines=c.readlines()\n",
    "for i in range(len(lines)):\n",
    "    comments.append(lines[i].replace(\"\\n\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",'').split(\",\"))\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19083"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19083/19083 [00:00<00:00, 87146.81it/s]\n"
     ]
    }
   ],
   "source": [
    "#build corpus with the single words appearing in all reviews\n",
    "corpus={}\n",
    "#update=1\n",
    "for i in tqdm(range(len(comments))):\n",
    "    #print(\"i={}\".format(i))\n",
    "    for j in range(len(comments[i])):\n",
    "        sentence=comments[i][j].split(\" \")\n",
    "        \n",
    "        for word in sentence:\n",
    "            \n",
    "            if word not in corpus:\n",
    "                corpus.update({word:0})\n",
    "                #update=update+1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36595"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19083/19083 [01:18<00:00, 241.71it/s]\n"
     ]
    }
   ],
   "source": [
    "#build vector with the count of words appearing in all reviews\n",
    "vectors=[]\n",
    "vec_sentence=[]\n",
    "doc=dict.fromkeys(corpus, 0)\n",
    "for i in tqdm(range(0,len(comments))):\n",
    "    vec_sentence=[]\n",
    "    for j in range(len(comments[i])):\n",
    "        \n",
    "        sentence=comments[i][j].split(\" \")\n",
    "        for word in sentence:\n",
    "            doc[word] += 1\n",
    "        \n",
    "        vec_sentence.append(list(doc.values()))\n",
    "        doc=dict.fromkeys(corpus, 0)\n",
    "    vectors.append(vec_sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36595\n",
      "19083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19083"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "print(len(comments[:][:]))\n",
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#from kneed import KneeLocator\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "for i in range(len(vectors)):\n",
    "    for j in range(len(vectors[i])):\n",
    "        X.append(vectors[i][j])\n",
    "X=np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def release_list(a):\n",
    "    del a[:]\n",
    "    del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_list(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(init=\"random\", n_clusters=2, n_init=30, max_iter=5000).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 0], dtype=int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13192 23493\n"
     ]
    }
   ],
   "source": [
    "y=(kmeans.labels_)\n",
    "num_zeros = (y == 0).sum()\n",
    "num_ones = (y == 1).sum()\n",
    "print(num_zeros,num_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are applying KMeans (k=2) to vectors belonging to $R^{36595}$. The results of the clustering clearly show a realistic result, but it's hard to quantify how accurate the result is.\\\n",
    "Probably a better way to solve the problem would be implementing a neural network, but it would take a lot of time to assign the labels to the comments (those are needed for the training). Or maybe it could also work to train the neural network with the results obtained from the clustering, but we can't estimate how accurate the KMeans is and the error related to this would carry over the neural network itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (SageMaker JumpStart Data Science 1.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:793310587911:image/sagemaker-jumpstart-data-science-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
